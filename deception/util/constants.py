# ENV
NUMBER_EPISODES = 100000    # agent training
EPISODES_WITH_HUMAN = 1     # games with human

# Robot type
TOM = 0
NO_TOM = 1
DECEPTION = 2
EXTERNAL_DECEPTION = 3
SUPERFICIAL_DECEPTION = 4
HIDDEN_DECEPTION = 5

# Constraint for first flip suggestion
CLICKS_UNTIL_MATCH_THRESHOLD_WITH_PAIR = 4
CLICKS_UNTIL_MATCH_THRESHOLD_WITHOUT_PAIR = 9

# Action
SUGGEST_NONE = 0
SUGGEST_ROW_COLUMN = 1
SUGGEST_CARD = 2

# Reward for suggestions
REWARD_SUGGEST_NONE = 10
REWARD_SUGGEST_RC =  0.1
REWARD_SUGGEST_CARD = 0.025

# Constants for game state
BEGIN_STATE = 3 
MIDDLE_STATE = 2 
END_STATE = 1

# Q-table states
INIT_STATE = 0

NO_HELP_BEG_F_CORRECT = 1
NO_HELP_BEG_F_WRONG = 2
NO_HELP_MID_F_CORRECT = 3
NO_HELP_MID_F_WRONG = 4
NO_HELP_END_F_CORRECT = 5
NO_HELP_END_F_WRONG = 6

SUGG_ROW_BEG_F_CORRECT = 7
SUGG_ROW_BEG_F_WRONG = 8
SUGG_ROW_MID_F_CORRECT = 9
SUGG_ROW_MID_F_WRONG  = 10
SUGG_ROW_END_F_CORRECT  = 11
SUGG_ROW_END_F_WRONG = 12

SUGG_CARD_BEG_F_CORRECT = 13
SUGG_CARD_BEG_F_WRONG = 14          # always 0: if the agent suggest the second card then it's 100% match
SUGG_CARD_MID_F_CORRECT = 15
SUGG_CARD_MID_F_WRONG = 16          # always 0
SUGG_CARD_END_F_CORRECT = 17
SUGG_CARD_END_F_WRONG = 18          # always 0

NO_HELP_BEG_S_CORRECT = 19
NO_HELP_BEG_S_WRONG = 20
NO_HELP_MID_S_CORRECT = 21
NO_HELP_MID_S_WRONG = 22
NO_HELP_END_S_CORRECT = 23
NO_HELP_END_S_WRONG = 24

SUGG_ROW_BEG_S_CORRECT = 25
SUGG_ROW_BEG_S_WRONG = 26
SUGG_ROW_MID_S_CORRECT = 27
SUGG_ROW_MID_S_WRONG  = 28
SUGG_ROW_END_S_CORRECT  = 29
SUGG_ROW_END_S_WRONG = 30

SUGG_CARD_BEG_S_CORRECT = 31
SUGG_CARD_BEG_S_WRONG = 32        
SUGG_CARD_MID_S_CORRECT = 33
SUGG_CARD_MID_S_WRONG = 34        
SUGG_CARD_END_S_CORRECT = 35
SUGG_CARD_END_S_WRONG = 36        